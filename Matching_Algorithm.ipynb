{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galavidar/lab-project/blob/main/Matching_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9svrQOtPTyY"
      },
      "source": [
        "## CV Job Matching using Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EF1ia-RPTyb"
      },
      "source": [
        "### Coding\n",
        "#### 1. Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDFUjQi3S171",
        "outputId": "103db136-b311-46c7-c738-8cecc52da744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Collecting playwright\n",
            "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.49.1 pyee-12.0.0\n",
            "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 37.4s\u001b[0K\u001b[1G161.3 MiB [] 0% 19.5s\u001b[0K\u001b[1G161.3 MiB [] 0% 13.4s\u001b[0K\u001b[1G161.3 MiB [] 0% 8.2s\u001b[0K\u001b[1G161.3 MiB [] 1% 7.1s\u001b[0K\u001b[1G161.3 MiB [] 1% 6.7s\u001b[0K\u001b[1G161.3 MiB [] 1% 6.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 5.7s\u001b[0K\u001b[1G161.3 MiB [] 2% 5.3s\u001b[0K\u001b[1G161.3 MiB [] 3% 5.3s\u001b[0K\u001b[1G161.3 MiB [] 3% 4.9s\u001b[0K\u001b[1G161.3 MiB [] 4% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 4% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 5% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 5% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 6% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 6% 4.4s\u001b[0K\u001b[1G161.3 MiB [] 7% 4.2s\u001b[0K\u001b[1G161.3 MiB [] 8% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 8% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 9% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 9% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 10% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 11% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 13% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 14% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 14% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 15% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 16% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 17% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 18% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 19% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 21% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 22% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 23% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 24% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 26% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 27% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 29% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 29% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 30% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 31% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 32% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 34% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 35% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 38% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 40% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 41% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 44% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 45% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 46% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 47% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 48% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 49% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 50% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 51% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 52% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 53% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 54% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 55% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 56% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 57% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 58% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 62% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 71% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 73% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 76% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 82% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 20.8s\u001b[0K\u001b[1G100.9 MiB [] 0% 12.1s\u001b[0K\u001b[1G100.9 MiB [] 0% 8.4s\u001b[0K\u001b[1G100.9 MiB [] 1% 5.0s\u001b[0K\u001b[1G100.9 MiB [] 1% 4.4s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.7s\u001b[0K\u001b[1G100.9 MiB [] 3% 3.3s\u001b[0K\u001b[1G100.9 MiB [] 4% 3.1s\u001b[0K\u001b[1G100.9 MiB [] 4% 3.0s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.8s\u001b[0K\u001b[1G100.9 MiB [] 6% 2.7s\u001b[0K\u001b[1G100.9 MiB [] 7% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 8% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 9% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 10% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 11% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 12% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 13% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 14% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 16% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 17% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 18% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 19% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 20% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 22% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 23% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 25% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 27% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 28% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 29% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 31% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 32% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 33% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 35% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 36% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 37% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 39% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 40% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 41% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 42% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 43% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 44% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 45% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 46% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 46% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 48% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 49% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 50% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 52% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 53% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 55% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 57% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 59% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 61% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 63% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 64% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 66% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 83% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 89% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 19.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 14.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 7.2s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.1s\u001b[0K\u001b[1G87.6 MiB [] 3% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 5% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 6% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 7% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 8% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 8% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 9% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 10% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 13% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 15% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 16% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 17% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 18% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 21% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 23% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 24% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 28% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 30% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 32% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 34% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 35% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 37% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 38% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 44% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 46% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 48% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 50% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 51% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 53% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 56% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 58% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 59% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 62% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 63% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 64% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 65% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 67% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 69% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 71% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 72% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 74% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 76% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 78% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 80% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 81% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 83% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 86% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 90% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 92% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 24.6s\u001b[0K\u001b[1G95.5 MiB [] 0% 14.7s\u001b[0K\u001b[1G95.5 MiB [] 0% 7.5s\u001b[0K\u001b[1G95.5 MiB [] 1% 3.7s\u001b[0K\u001b[1G95.5 MiB [] 2% 3.5s\u001b[0K\u001b[1G95.5 MiB [] 2% 3.9s\u001b[0K\u001b[1G95.5 MiB [] 3% 3.4s\u001b[0K\u001b[1G95.5 MiB [] 4% 3.0s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.9s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.8s\u001b[0K\u001b[1G95.5 MiB [] 6% 2.7s\u001b[0K\u001b[1G95.5 MiB [] 7% 2.6s\u001b[0K\u001b[1G95.5 MiB [] 8% 2.4s\u001b[0K\u001b[1G95.5 MiB [] 10% 2.2s\u001b[0K\u001b[1G95.5 MiB [] 11% 2.3s\u001b[0K\u001b[1G95.5 MiB [] 11% 2.4s\u001b[0K\u001b[1G95.5 MiB [] 12% 2.2s\u001b[0K\u001b[1G95.5 MiB [] 13% 2.3s\u001b[0K\u001b[1G95.5 MiB [] 14% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 15% 2.0s\u001b[0K\u001b[1G95.5 MiB [] 16% 2.0s\u001b[0K\u001b[1G95.5 MiB [] 17% 2.0s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.9s\u001b[0K\u001b[1G95.5 MiB [] 20% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 27% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 29% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 30% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 31% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 31% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 32% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 34% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 35% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 36% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 37% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 38% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 39% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 40% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 41% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 42% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 43% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 44% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 45% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 46% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 48% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 49% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 51% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 53% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 54% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 56% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 58% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 59% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 61% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 62% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 65% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 67% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 68% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 69% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 71% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 73% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 78% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 80% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 83% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 86% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 3% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 12% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 30% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 75% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Install all dependencies\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install playwright\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpZnVCxZSQ8a",
        "outputId": "be95a524-dd84-442f-8389-c46e22f37eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import libraries\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument,Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy.linalg import norm\n",
        "from termcolor import colored\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "nltk.download('all', download_dir='/usr/local/lib/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5CoVK6-iSWsU"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "jobs = pd.read_csv('/content/linkedin_tech_82k_git.csv')\n",
        "profiles=pd.read_csv('/content/Lab_project.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57rqnCNPTyi"
      },
      "source": [
        "Keep only some columns to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BtpytBBCPTyj"
      },
      "outputs": [],
      "source": [
        "jobs =jobs[['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'months_experience', 'post_url', 'salary', 'title']]\n",
        "\n",
        "profiles=profiles[['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'url', 'volunteer_experience', 'сourses']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UazPSTMlPTyj",
        "outputId": "07bb0614-8d58-4466-acb1-d69849557a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                data\n",
            "0  Full-time Broadcast Media Mid-Senior level Cyb...\n",
            "1  Full-time Hospital & Health Care, Medical Devi...\n",
            "2  Full-time Computer Hardware, Computer Software...\n",
            "3  Full-time Computer Hardware, Computer Software...\n",
            "4  Full-time Computer Hardware, Computer Software...\n"
          ]
        }
      ],
      "source": [
        "# Create a new column called 'data' and merge the values of the other columns into it\n",
        "jobs['data'] = jobs[['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'months_experience', 'post_url', 'salary', 'title']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "# Drop the individual columns if you no longer need them\n",
        "jobs.drop(['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'months_experience', 'post_url', 'salary', 'title'], axis=1, inplace=True)\n",
        "# Preview the updated dataframe\n",
        "print(jobs.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column called 'data' and merge the values of the other columns into it\n",
        "profiles['data'] = profiles[['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'url', 'volunteer_experience', 'сourses']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "# Drop the individual columns if you no longer need them\n",
        "profiles.drop(['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'url', 'volunteer_experience', 'сourses'], axis=1, inplace=True)\n",
        "# Preview the updated dataframe\n",
        "print(profiles.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Csl_zlCozWc",
        "outputId": "d88514b8-8237-4a01-d48a-4afa45d287fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                data\n",
            "0  [] {\"company_id\":null,\"industry\":\"Michigan Con...\n",
            "1  [] {\"company_id\":\"bcbsks\",\"industry\":\"Blue Cro...\n",
            "2  [] {\"company_id\":null,\"industry\":\"Holm Corruga...\n",
            "3  [{\"meta\":\"Issued Sep 2021 See credential\",\"sub...\n",
            "4  [] {\"company_id\":\"new-york-college-of-traditio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ntmC8RPTyj"
      },
      "source": [
        "#### 3. Tokenize data\n",
        "We tokenize the words in the 'data' column and tag them with unique identifiers using the TaggedDocument class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sCAMBUD8Sorw"
      },
      "outputs": [],
      "source": [
        "jobs_data = list(jobs['data'])\n",
        "user_data=(list(profiles['data']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_data(text):\n",
        "    \"\"\"\n",
        "    Tokenize and clean text by removing stopwords, punctuation, and non-alphabetic tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    filtered_tokens = [\n",
        "        word for word in tokens\n",
        "        if word.isalpha() and word not in stop_words  # Keep only alphabetic words and non-stopwords\n",
        "    ]\n",
        "    return filtered_tokens\n"
      ],
      "metadata": {
        "id": "eJuLxLnfsvFK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and tag profiles\n",
        "tagged_profiles = [\n",
        "    TaggedDocument(words=preprocess_data(profile), tags=[f\"profile_{i}\"])\n",
        "    for i, profile in enumerate(user_data)\n",
        "]\n",
        "\n",
        "# Preprocess and tag jobs\n",
        "tagged_jobs = [\n",
        "    TaggedDocument(words=preprocess_data(job), tags=[f\"job_{i}\"])\n",
        "    for i, job in enumerate(jobs_data)\n",
        "]\n",
        "\n",
        "# Combine both lists\n",
        "tagged_data = tagged_profiles + tagged_jobs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Sfcsm8jTdmMK",
        "outputId": "6e12530f-3207-49b9-a987-83fe74202b6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8f894372087d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Preprocess and tag jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m tagged_jobs = [\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"job_{i}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8f894372087d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Preprocess and tag jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m tagged_jobs = [\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"job_{i}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-8-7b494f0f446b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTokenize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclean\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mby\u001b[0m \u001b[0mremoving\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malphabetic\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tokenize and convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     filtered_tokens = [\n\u001b[1;32m     14\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[1;32m    142\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     return [\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Preprocess and tokenize profiles and jobs\n",
        "tokenized_profiles = [preprocess_data(profile) for profile in user_data]\n",
        "tokenized_jobs = [preprocess_data(job) for job in jobs_data]\n",
        "\n",
        "# Combine all tokenized data\n",
        "tokenized_data = tokenized_profiles + tokenized_jobs\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, min_count=2, epochs=10, workers=4)\n",
        "\n",
        "# Get the vocabulary keys\n",
        "keys = model.wv.key_to_index.keys()\n",
        "\n",
        "# Print the length of the vocabulary keys\n",
        "print(len(keys))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn9VUH3Yhpmw",
        "outputId": "43e41c0b-d649-4bbd-ca58-083731e5851d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pN_H6onBTamK"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(vector_size=100, min_count=2, epochs=10, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Sv_Mtx4OWFgF",
        "outputId": "4509438e-e16c-4dbf-eb58-dd5783bd29f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3e9fe48d9a79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Vocabulary building\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the vocabulary keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the length of the vocabulary keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    492\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    568\u001b[0m                 )\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "# Vocabulary building\n",
        "model.build_vocab(tagged_data)\n",
        "# Get the vocabulary keys\n",
        "keys = model.wv.key_to_index.keys()\n",
        "# Print the length of the vocabulary keys\n",
        "print(len(keys))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBk5-LZLPTyk"
      },
      "source": [
        "#### 5. Train and save the model\n",
        "Train the model on tagged data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0jwx4eNAWYrI",
        "outputId": "6504e97a-58e5-4ebb-afc2-a67485db184f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 10/10\n",
            "Model saved\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for epoch in range(model.epochs):\n",
        "    print(f\"Training epoch {epoch+1}/{model.epochs}\")\n",
        "# Train for all desired epochs in a single call\n",
        "    model.train(\n",
        "    tokenized_data,\n",
        "    total_examples=model.corpus_count,\n",
        "    epochs=10,  # Set the total number of training epochs\n",
        "    start_alpha=0.025,\n",
        "    end_alpha=0.0001\n",
        ")\n",
        "\n",
        "model.save('cv_job_maching.model')\n",
        "print(\"Model saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFXCiT8GWgdP"
      },
      "source": [
        "#### 6. Inputs of CV and JD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile_experiance= \"\"\"Company Name: Next Career\n",
        "Duration at company: 2 yrs 1 mo\n",
        "Job Name: Headhunter (via Next Career)\n",
        "Company: Flex\n",
        "Job Type: Contract\n",
        "Job Duration: Apr 2024 - May 2024 · 2 mos\n",
        "Job Location: Israel · Hybrid\n",
        "Additional Content: Recruitment as a Service\n",
        "Skills: N/A\n",
        "--------------------------------------------------\n",
        "Job Name: Headhunter (via Next Career)\n",
        "Company: Puzzle Projects LTD\n",
        "Job Type: Contract\n",
        "Job Duration: Feb 2024 - Apr 2024 · 3 mos\n",
        "Job Location: Israel · Remote\n",
        "Additional Content: Recruitment as a service for airspace and defence company.\n",
        "Skills: N/A\n",
        "--------------------------------------------------\n",
        "Job Name: Talent Acquisition Manager\n",
        "Company: eBay & Bitfury (via Contract)\n",
        "Job Type: Full-time\n",
        "Job Duration: 2017 - 2019 · 2 yrs\n",
        "Job Location: Israel\n",
        "Additional Content: Managing the full-cycle recruiting process through an outsourcing company.Conducting phone interviews and face-to-face HR interviews.Experience in global recruitment (EMEA, APAC).Proven experience with sourcing techniques: LinkedIn recruiter corporate, amazing-hiring, X-ray search, Social media.Working closely with hiring managers and stakeholders and leading the hiring process.Hiring for Different RnD roles: DevOps, Backend development, and more.\n",
        "Skills: Communication · Hiring · Recruiting · Strategic Sourcing · Sourcing\n",
        "--------------------------------------------------\n",
        "Job Name: Senior Talent Acquisition Specialist\n",
        "Company: Israeli Navy\n",
        "Job Type: Full-time\n",
        "Job Duration: 2016 - 2017 · 1 yr\n",
        "Job Location: Israel\n",
        "Additional Content: In charge of recruitment to classified sections in the Israeli navy, units such as Mafteh.Sourcing & Hiring candidates mostly from a technological background in fields of Backend Development: (Java, Python, NodeJS, GO), FrontEnd: (HTML, Angular, JS), DevOps, Different IT positions.\n",
        "Skills: N/A\n",
        "--------------------------------------------------\"\"\"\n",
        "\n",
        "profile_education=\"\"\"\n",
        "Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2019 - 2021\n",
        "  Description: Master's degree, Organizational Behavior StudiesMaster's degree, Organizational Behavior Studies\n",
        "  Skills:\n",
        "  Additional Text: Grade: 94\n",
        "Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2012 - 2014\n",
        "  Description: Bachelor's degree, Human Resources Management and ServicesBachelor's degree, Human Resources Management and Services\n",
        "  Skills:\n",
        "  Additional Text: Grade: 95\n",
        "Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2012 - 2014\n",
        "  Description: Bachelor's degree, National Security Policy StudiesBachelor's degree, National Security Policy Studies\n",
        "  Skills:\n",
        "  Additional Text:\n",
        " \"\"\"\n",
        "\n",
        "profile_skills=\"\"\"Unique skills: ['Analytical Skills', 'Candidate Management', 'Co-sourcing', 'Communication', 'Corporate Recruiting', 'Curriculum Vitae (CV)', 'English', 'Executive Search', 'Game Theory', 'Hiring', 'Human Resources (HR)', 'Interpersonal Skills', 'Interview Preparation', 'Leadership', 'LinkedIn', 'Microsoft Excel', 'Problem Solving', 'Recruiting', 'Recruitment-to-Recruitment', 'Resume Writing', 'Sales', 'Sourcing', 'Start-up Ventures', 'Statistical Data Analysis', 'Statistics', 'Strategic Sourcing', 'Strategy', 'Vacancies', 'Venture Capital']]\"\"\""
      ],
      "metadata": {
        "id": "-yc-7RmxysmP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_job_posting(job_url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "        page = await context.new_page()\n",
        "\n",
        "        await page.goto(job_url)\n",
        "        await asyncio.sleep(5)  # Wait for 5 seconds\n",
        "\n",
        "        # Saving the page's HTML content for debugging purposes\n",
        "        html = await page.content()\n",
        "        with open(\"page_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html)\n",
        "        print(\"Saved page content to page_debug.html\")\n",
        "\n",
        "        # Taking a screenshot of the page for verification\n",
        "        await page.screenshot(path=\"screenshot.png\", full_page=True)\n",
        "        print(\"Screenshot saved as screenshot.png\")\n",
        "\n",
        "\n",
        "        # Extract job title\n",
        "        job_title = await page.inner_text('h1.top-card-layout__title')  # Adjust selector\n",
        "        company = await page.inner_text('a.topcard__org-name-link')  # Adjust selector\n",
        "        location = await page.inner_text('span.topcard__flavor--bullet')  # Adjust selector\n",
        "        description = await page.inner_text('div.show-more-less-html__markup')  # Adjust selector\n",
        "\n",
        "        print(f\"Job Title: {job_title}\")\n",
        "        print(f\"Company: {company}\")\n",
        "        print(f\"Location: {location}\")\n",
        "        print(f\"Description: {description}\")\n",
        "\n",
        "        await browser.close()\n",
        "        return job_title, company, location, description\n",
        "\n",
        "job_url = \"https://www.linkedin.com/jobs/view/4119256367\"\n",
        "job_title, company, location, description = await scrape_job_posting(job_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZ2K0sdTDeB",
        "outputId": "77eaaa4f-3576-422d-9489-d1ea9179bb98"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved page content to page_debug.html\n",
            "Screenshot saved as screenshot.png\n",
            "Job Title: Recruitment Lead\n",
            "Company: Logistics company (NDA) \n",
            "Location:  Tel Aviv District, Israel\n",
            "Description: HR Operations (Recruitment Lead)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We're seeking an HR Operations Manager in Tel Aviv.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You will be responsible for:\n",
            "\n",
            "• Leading the recruitment team and overseeing mass recruitment for our business units\n",
            "\n",
            "• Managing new-hire orientation and onboarding\n",
            "\n",
            "Analyzing and optimizing business processes, advising on local law, and collaborating with providers\n",
            "\n",
            "• Implementing an effective onboarding system\n",
            "\n",
            "• Working with team metrics and KPIs\n",
            "\n",
            "• Developing a high-quality recruitment strategy and structure\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You might be a fit if you:\n",
            "\n",
            "• Have solid relevant experience in talent development or recruitment\n",
            "\n",
            "• Have extensive knowledge of local employment law\n",
            "\n",
            "• Are a strong team player\n",
            "\n",
            "• Are experienced in supporting various HR projects and drafting or amending policies based on business needs\n",
            "\n",
            "• Are able to comfortably deal with ambiguity, interpret non-verbal communication, and understand cultural nuances\n",
            "\n",
            "• Are able to work independently and effectively in a fast-paced and changing environment\n",
            "\n",
            "• Possess excellent verbal and written communication skills\n",
            "\n",
            "• Are proficient in Hebrew and English\n",
            "\n",
            "• Have strong skills in Microsoft Office, especially Excel\n",
            "\n",
            "• Have experience in team management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIElcDVCPTym"
      },
      "source": [
        "- **Develop a function to pre-process input text**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parts = [profile_experiance, profile_education, profile_skills]\n",
        "profile_combined = \" \".join(parts)\n",
        "\n",
        "parts2 = [job_title, company, location, description]\n",
        "jobs_combined = \" \".join(parts2)"
      ],
      "metadata": {
        "id": "IvYbaOjP4WEM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nY1-Fn97WgoN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Ensure the input is a string (not tokenized yet)\n",
        "    if isinstance(text, list):  # If it's already tokenized, return as-is\n",
        "        return text\n",
        "    # Lowercase and remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J4TR1IklWqp8"
      },
      "outputs": [],
      "source": [
        "# Apply to CV and JD\n",
        "input_CV = preprocess_text(profile_combined)\n",
        "input_JD = preprocess_text(jobs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_tfidf_weighted_vector(model, document, tfidf_weights):\n",
        "    words = document.split()\n",
        "    word_vectors = [model.wv[word] * tfidf_weights.get(word, 0) for word in words if word in model.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Example TF-IDF setup\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit([input_CV, input_JD])  # Fit on both the CV and Job Description\n",
        "tfidf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RTEot4czobsp",
        "outputId": "b27a8e6d-b0d0-4156-f40c-9262bb66f7c5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-175ed10d5831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Example TF-IDF setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_CV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_JD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fit on both the CV and Job Description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtfidf_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         )\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Pretrained model for semantic similarity\n",
        "v1 = model.encode(input_CV)\n",
        "v2 = model.encode(input_JD)\n",
        "\n",
        "similarity = cosine_similarity([v1], [v2])[0][0] * 100\n",
        "print(f\"Similarity: {round(similarity, 2)}\")\n"
      ],
      "metadata": {
        "id": "NMBNi-LNoqiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7dKopfNPTym"
      },
      "source": [
        "#### 7. Matching\n",
        "Using the trained model, we infer the document vectors for the resume and job description. Then, we calculate the cosine similarity between the two vectors to determine the match between the resume and the job description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qlbOvGLfWqsd",
        "outputId": "0a862c7e-d181-4b9c-d7a9-2d8eab077bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2Vec' object has no attribute 'infer_vector'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-708763872e74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv_job_maching.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_CV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_JD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'infer_vector'"
          ]
        }
      ],
      "source": [
        "# Model evaluation\n",
        "model = Doc2Vec.load('cv_job_maching.model')\n",
        "v1 = model.infer_vector(input_CV.split())\n",
        "v2 = model.infer_vector(input_JD.split())\n",
        "similarity = 100*(np.dot(np.array(v1), np.array(v2))) / (norm(np.array(v1)) * norm(np.array(v2)))\n",
        "print(round(similarity, 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load('cv_job_maching.model')\n",
        "\n",
        "def get_document_vector(model, document):\n",
        "    \"\"\"\n",
        "    Compute the document vector by averaging word vectors.\n",
        "    Words not in the model's vocabulary are ignored.\n",
        "    \"\"\"\n",
        "    words = document.split()\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if not word_vectors:  # Handle empty or unknown words\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Compute document vectors for the input CV and Job Description\n",
        "v1 = get_document_vector(model, input_CV)\n",
        "v2 = get_document_vector(model, input_JD)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = 100 * np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "print(round(similarity, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSHVg203l68o",
        "outputId": "05220ce0-15f9-4704-942e-0520d88912b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"v1 (CV vector):\", v1)\n",
        "print(\"v2 (JD vector):\", v2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHiCrZpwmYYP",
        "outputId": "c29a36cc-cc0c-4ac3-eaa9-fd8c3db849e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1 (CV vector): [ 0.21321434 -0.22142796  0.2116971  -0.33446565 -0.4262473  -0.04524133\n",
            "  0.19885267 -0.16503912 -0.26369074 -0.15190415  0.42369696  0.21966852\n",
            "  0.03887178  0.08007418  0.21436514 -0.11143193 -0.59838694  0.26307216\n",
            " -0.16932689  0.09004417 -0.76221895  0.17477512  0.2535282  -0.86055857\n",
            " -0.25286117 -0.26593503  0.51129496 -0.058488    0.9443745  -0.4605357\n",
            "  0.19516094  0.00518642 -0.50468737 -0.4272709  -0.24756731 -0.46951267\n",
            "  0.18340693  0.2840015  -0.14321882  0.4279297  -0.0746206  -0.00127734\n",
            " -0.26268497 -0.5482474  -0.79058087 -0.2576207  -0.37958285 -0.6920545\n",
            " -0.01455264 -0.21593226  0.06846645 -0.01554809  0.23201512  0.19747488\n",
            "  0.75052404  0.1777384   0.52493984  0.22536877  0.43757173  0.45702225\n",
            " -0.8085681   0.02853793 -1.0276564   0.37121996  0.19669098 -0.20566857\n",
            "  0.59110284 -0.08170218  0.17980243  0.0768702  -0.0112029  -0.07267099\n",
            " -0.09340509 -0.70791996  0.4260606  -0.01980962 -0.31986737  0.4580304\n",
            "  0.709492    0.05514605  0.12851556 -0.5273869   0.07781549  0.10466715\n",
            " -0.49342963  0.12896883 -0.07582209  0.17473455 -0.13152844 -0.00744711\n",
            " -0.6887933   0.64475477 -0.18534699 -0.35450834 -0.18016444  0.16594583\n",
            " -0.42965925 -0.61085933 -0.10011583 -0.49891135]\n",
            "v2 (JD vector): [ 0.6277033   0.41867623  0.05048237  0.5912166   0.55204797  0.08169106\n",
            " -0.04841744  0.32178494  0.31897375 -0.2673963  -0.09364168  0.05508843\n",
            "  0.31055942 -0.05530115  0.23837058  0.10725778  0.657665    0.39381638\n",
            " -0.23907006 -0.01670596  0.2527695   0.2794137  -0.84569496  0.08230843\n",
            " -0.2405454  -0.23856756 -0.84850895 -0.46505222 -0.61166865  0.68832344\n",
            " -0.6178908  -0.84762996  0.10703195 -0.4813297  -0.12330844  0.26105422\n",
            "  0.6646597   0.18387921  0.17143042 -0.66588074  0.29112872  0.32848027\n",
            " -0.17076336 -0.19903556  0.25779888  0.14185944 -0.5241953  -0.6393431\n",
            " -0.07196296  0.14413314 -0.6076988  -0.22549993 -0.09237686 -0.01286129\n",
            " -0.07127365 -0.36576757  0.52263206 -0.1759395   0.62660563  0.5244119\n",
            " -0.09187195  0.20443393  0.6466864  -0.46953985  0.03088121  0.6509249\n",
            "  0.01292919 -0.06881076 -0.08802599 -0.688276    0.21691029  0.3378353\n",
            " -0.44030878  0.00116197 -0.7811068   0.30695927 -0.1089398  -0.27404812\n",
            "  0.6807782  -0.12182747 -1.037753   -0.36701852 -0.03492224  0.28702572\n",
            " -0.00121337  0.12346847 -0.1707118   0.5312619   0.40999988 -0.5539879\n",
            " -0.00999602  0.16128083 -0.4305482   0.95557135  0.06008727 -0.2829022\n",
            " -0.22611609  0.5786759   0.03606605  0.56253994]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFDN3UbLPTyn"
      },
      "source": [
        "#### 8. Visualization and Notification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cCjPrtxYPTyn",
        "outputId": "abd4c534-592f-4617-e556-ea69f1de8d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"68bad900-c0bf-4263-9101-fa327ba0532f\" class=\"plotly-graph-div\" style=\"height:400px; width:600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"68bad900-c0bf-4263-9101-fa327ba0532f\")) {                    Plotly.newPlot(                        \"68bad900-c0bf-4263-9101-fa327ba0532f\",                        [{\"domain\":{\"x\":[0,1],\"y\":[0,1]},\"gauge\":{\"axis\":{\"range\":[0,100]},\"steps\":[{\"color\":\"#FFB6C1\",\"range\":[0,50]},{\"color\":\"#FFFFE0\",\"range\":[50,70]},{\"color\":\"#90EE90\",\"range\":[70,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":100}},\"mode\":\"gauge+number\",\"title\":{\"text\":\"Matching percentage (%)\"},\"value\":14.937148449734487,\"type\":\"indicator\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"width\":600,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('68bad900-c0bf-4263-9101-fa327ba0532f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low chance, need to modify your CV!\n"
          ]
        }
      ],
      "source": [
        "# Visualization\n",
        "fig = go.Figure(go.Indicator(\n",
        "    domain = {'x': [0, 1], 'y': [0, 1]},\n",
        "    value = similarity,\n",
        "    mode = \"gauge+number\",\n",
        "    title = {'text': \"Matching percentage (%)\"},\n",
        "    #delta = {'reference': 100},\n",
        "    gauge = {\n",
        "        'axis': {'range': [0, 100]},\n",
        "        'steps' : [\n",
        "            {'range': [0, 50], 'color': \"#FFB6C1\"},\n",
        "            {'range': [50, 70], 'color': \"#FFFFE0\"},\n",
        "            {'range': [70, 100], 'color': \"#90EE90\"}\n",
        "        ],\n",
        "             'threshold' : {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 100}}))\n",
        "\n",
        "fig.update_layout(width=600, height=400)  # Adjust the width and height as desired\n",
        "fig.show()\n",
        "\n",
        "# Print notification\n",
        "if similarity < 50:\n",
        "    print(colored(\"Low chance, need to modify your CV!\", \"red\", attrs=[\"bold\"]))\n",
        "elif similarity >= 50 and similarity < 80:\n",
        "    print(colored(\"Good chance but you can improve further!\", \"yellow\", attrs=[\"bold\"]))\n",
        "else:\n",
        "    print(colored(\"Excellent! You can submit your CV.\", \"green\", attrs=[\"bold\"]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}